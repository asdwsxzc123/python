""" 算法分类 """
监督学习(预测):特征值和目标值,有标准答案
分类 K-近邻算法,贝叶斯分类,决策树,随机森林,逻辑回归,神经网络
回归: 线性回归,岭回归
标注: 隐马尔可夫模型


无监督学习: 特征值, 1000个样本
聚类: k-means

分类: 目标值离散型
回归: 目标值连续型

""" 数据来源 """
1. 公司自有数据
2. 合作过来的数据
3. 购买的数据

原始数据:
1. 明确问题做什么,建立模型                                                                                               
2.数据的基本处理:pandas处理数据(缺失值,合并表)
3. 特征工程(特征进行处理)
分类:
回归:
模型:算法+数据
4.找到合适算法去进行预测
5.模型的评估,判定效果

如果没有合适的:
1.换算法 参数
2.特征工程,循环

import sklearn

""" k-近邻算法 """
# 需要做标准化处理,因为单个特征的值的大小会影响权重
# 如何求两个样本的距离:欧式距离公司
# 根号((a1-b1)^2 + (a2-b2)^2)
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
n_neighbors:int (使用的邻居数)
algorithm: 可选用于计算最近邻居的算法, 'ball_tree'使用BallTree, 'auto'将尝试根据传递给fit方法的值来绝对最合适的算法

优点:
    简单,易于理解,易于实现,无序估计参数,无需训练

缺点: 
    懒惰算法,对测试样本分类是的计算量大,内存开销大
    必须指定k,k值选择不当测分类精度不能保证
1.k值取多大
    k值取很小: 容易受异常点影响
    k值取很大: 容易受K值数量(类别)波动,比例容易受影响
2.性能问题
    每一个样本都需要与10w样本做比较,时间复杂度n^n

使用场景: 小场景使用,几千-几万样本,具体场景具体业务去测试

加快搜索速度基于算法的感觉KDTree

""" 朴素贝叶斯算法 """
1. 概率基础
# 每个文档划分到每个类别的概率,
2. 朴素贝叶斯概述
# 联合概率
P(A,B) = P(A)P(B)
# 条件概率
# a在b的条件下
P(A|B)
P(A1,A2|B) = P(A1|B)P(A2|B)
# 注意:A1,A2相互独立
P(科技|词1,词2,词3)

贝叶斯公式
P(C|W) = P(W|C)P(C)/P(W)
# w为文档的特征值(频数统计,预测文档提供), c为文档类别
P(C):每个文档类别的概率（某文档类别数/总文档数量)
P(W|C): 给定 类别下特征(被预测文档中出现的词的评率)

# 拉普拉斯平滑系数
有些例子的概率可能是0,但这是不合理的,因此需要加个系数
所有的分子和分母都加1
# sklearn.naive_bayes.MultinomialNB(alpha=1.0)

缺点:
    假设文章中的一些词是独立没关系的,不靠谱
    训练集当中去进行统计词这些工作 会对结果造成干扰
优点:
    文本分类,现在使用神经网络

""" 分类模型的评估 """
一般使用准确率
混淆矩阵
预测结果与正确标记之间存在四种不同的组合,构成混淆矩阵

# precision, recall
精确率和召回率
召回率: 正式为整理的样本中预测结果为正数
    sklearn.metrics.classification_report(y_true, y_pred, target_names = None)
y_true:真实目标值
y_pred: 估计器预测目标值
target_names: 目标类别名称
return : 每个类别精准率与召回率

""" 模型的选择和调优 """
1.交叉验证
为了让被评估的模型更加可信
数据分为训练集和验证集,所有数据分成n等分,得出每个数据的准确率,然后求平均值
2.网格搜索(超参数搜索)
很多参数需要手动指定,但手动过程复杂,因此需要设置几种参数集组合,每组超参数都采用交叉验证进行评估,最后选出最优参数组合建立模型
sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
    estimator:估计器
    param_grid: 估计器参数
    cv: 指定几折交叉验证
    fit: 输入训练数据
    score
    # 结果分析
    best_score
    best_estimator: 最好的参数模型
    cv_results:每次交叉验证的结果

""" 决策树 """
1.介绍
就是使用if else做判断,场景: 银行贷款
决策的划分
信息的度量和作用
# 信息熵,可以做文件压缩
H(D) = -(9/15log9/15 + 6/15log(6/15))
决策树: 信息增益
特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D),给定条件下D的信息条件熵H(D|A)之差,及公式为:
g(D,年龄) = H(D) - H(D'|年龄)
          = 0.971 - [1/3H(青年) + 1/3H(中年) + 1/3H(老年)]
            H(青年) = -(2/5log(2/5) + 3/5log(3/5))
            H(中年) = -(2/5log(2/5) + 3/5log(3/5))
            H(老年) = -(4/5log(4/5) + 1/5log(1/5))
常见决策树的算法
分类的依据: 
信息增益
基尼系数: 划分更加自信
ID3 信息增益最大准则
C4.5 信息增益比最大原则
CART
回归数: 平方误差最小
分类树: 基尼系数 最小准则,在sklearn中默认原则

DecisionTreeClassifier
max_depth: 深度
criterion : 'gini', 'entropy'
random_state: 随机种子
method
decision_path: 返回决策树的路径
tree.export_graphviz(estimator, out_file='tree.dot', feature_names=[','])
打开dot文件,需要安装graphviz,将dot文件转换为pdf.png
dot -Tpng tree.dot -o tree.png

优点:
简单的理解和解释,树木可视化
需要很少的数据准备,其他技术通常需要数据归一化
缺点:
决策树学习者可以创建不能很好地推广数据过于复杂的数,过拟合
改进:
减枝cart算法(决策树API当中已经实现,随机森林调优有关)
注: 企业重要决策,由于决策树很好的分析能力,在决策过程应用较多

""" 随机森林 """
包含了多个决策树,,会建立多个决策树的过程
N个样本,M个特征
单个树的建立过程:
1. 随机在n个样本中选择一个样本,重复N次, 样本可能重复
2. 随机在M个特征当中选出m个特征, m取值
建立10颗决策树,样本,特征大多不一样 随机又放回的抽样 bootstrap
随机抽样: 训练的结果不一样
放回抽样: 
sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=5,bootstrap=True,random_state=10,max_features='auto')
n_estimators: 随机选择器 integer, optional 森林里的树木数量120,200,300,500
criterion: 分割特征的测量方法
bootstrap: True,是否放回
max_features: 'auto',每个决策树的最大特征数量

优点:
1. 极好的准确率
2. 能够有效的运行在大数据集上
3. 能够处理高纬特征的输入样本,而且不需要降维
4. 能够评估各个特征在分类问题上的重要性